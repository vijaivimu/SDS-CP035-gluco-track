# ğŸ”´ GlucoTrack â€“ Advanced Track

## âœ… Week 1: Exploratory Data Analysis (EDA)

---

### ğŸ“¦ 1. Data Integrity & Structure

Q: Are there any missing, duplicate, or incorrectly formatted entries in the dataset?  
A:  

Q: Are all data types appropriate (e.g., numeric, categorical)?  
A:  

Q: Did you detect any constant, near-constant, or irrelevant features?  
A:  

---

### ğŸ¯ 2. Target Variable Assessment

Q: What is the distribution of `Diabetes_binary`?  
A:  

Q: Is there a class imbalance? If so, how significant is it?  
A:  

Q: How might this imbalance influence your choice of evaluation metrics or model strategy?  
A:  

---

### ğŸ“Š 3. Feature Distribution & Quality

Q: Which numerical features are skewed or contain outliers?  
A:  

Q: Did any features contain unrealistic or problematic values?  
A:  

Q: What transformation methods (if any) might improve these feature distributions?  
A:  

---

### ğŸ“ˆ 4. Feature Relationships & Patterns

Q: Which categorical features (e.g., `GenHealth`, `PhysicalActivity`, `Smoking`) show visible patterns in relation to `Diabetes_binary`?  
A:  

Q: Are there any strong pairwise relationships or multicollinearity between features?  
A:  

Q: What trends or correlations stood out during your analysis?  
A:  

---

### ğŸ§° 5. EDA Summary & Preprocessing Plan

Q: What are your 3â€“5 biggest takeaways from EDA?  
A:  

Q: Which features will you scale, encode, or exclude in preprocessing?  
A:  

Q: What does your cleaned dataset look like (rows, columns, shape)?  
A:  

---
---

## âœ… Week 2: Feature Engineering & Deep Learning Prep

---

### ğŸ·ï¸ 1. Categorical Feature Encoding

Q: Which categorical features in the dataset have more than two unique values?  
A:  

Q: Apply integer-encoding to these high-cardinality features. Why is this strategy suitable for a subsequent neural network with an embedding layer?  
A:  

Q: Display the first 5 rows of the transformed data to show the new integer labels.  
A:  

---

### âš–ï¸ 2. Numerical Feature Scaling

Q: Which numerical features did your EDA from Week 1 suggest would benefit from scaling?  
A:  

Q: Apply a scaling technique to these features. Justify your choice of `StandardScaler` vs. `MinMaxScaler` or another method.  
A:  

Q: Show the summary statistics of the scaled data to confirm the transformation was successful.  
A:  

---

### âœ‚ï¸ 3. Stratified Data Splitting

Q: Split the data into training, validation, and testing sets (e.g., 70/15/15). What function and parameters did you use?  
A:  

Q: Why is it critical to use stratification for this specific dataset?  
A:  

Q: Verify the stratification by showing the class distribution of `Diabetes_binary` in each of the three resulting sets.  
A:  

---

### ğŸ“¦ 4. Deep Learning Dataset Preparation

Q: Convert your three data splits into PyTorch `DataLoader` or TensorFlow `tf.data.Dataset` objects. What batch size did you choose and why?  
A:  

Q: To confirm they are set up correctly, retrieve one batch from your training loader. What is the shape of the features (X) and labels (y) in this batch?  
A:  

Q: Explain the role of the `shuffle` parameter in your training loader. Why is this setting important for the training set but not for the validation or testing sets?  
A:

---

## âœ… Week 3: Neural Network Design & Baseline Training

---

### ğŸ—ï¸ 1. Neural Network Architecture

Q: How did you design your baseline Feedforward Neural Network (FFNN) architecture?  
A:  

Q: What was your rationale for the number of layers, units per layer, and activation functions used?  
A:  

Q: How did you incorporate Dropout, Batch Normalization, and ReLU in your model, and why are these components important?  
A:  

---

### âš™ï¸ 2. Model Training & Optimization

Q: Which loss function and optimizer did you use for training, and why are they suitable for this binary classification task?  
A:  

Q: How did you monitor and control overfitting during training?  
A:  

Q: What challenges did you face during training (e.g., convergence, instability), and how did you address them?  
A:  

---

### ğŸ“ˆ 3. Experiment Tracking

Q: How did you use MLflow (or another tool) to track your deep learning experiments?  
A:  

Q: What parameters, metrics, and artifacts did you log for each run?  
A:  

Q: How did experiment tracking help you compare different architectures and training strategies?  
A:  

---

### ğŸ§® 4. Model Evaluation

Q: Which metrics did you use to evaluate your neural network, and why are they appropriate for this problem?  
A:  

Q: How did you interpret the Accuracy, Precision, Recall, F1-score, and AUC results?  
A:  

Q: Did you observe any trade-offs between metrics, and how did you decide which to prioritize?  
A:  

---

### ğŸ•µï¸ 5. Error Analysis

Q: How did you use confusion matrices or ROC curves to analyze your modelâ€™s errors?  
A:  

Q: What types of misclassifications were most common, and what might explain them?  
A:  

Q: How did your error analysis inform your next steps in model improvement?  
A:  

---

### ğŸ“ 6. Model Selection & Insights

Q: Based on your experiments, which neural network configuration performed best and why?  
A:  

Q: What are your top 3â€“5 insights from neural network development and experimentation?  
A:  

Q: How would you communicate your modelâ€™s strengths and limitations to a non-technical stakeholder?  
A: